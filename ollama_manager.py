# ollama_manager.py
from __future__ import annotations

import os
import subprocess
import time
from pathlib import Path
from typing import Final

import requests

OLLAMA_HOST: Final[str] = os.getenv("OLLAMA_HOST", "http://localhost:11434")
CHECK_URL: Final[str] = f"{OLLAMA_HOST}/api/tags"


def _is_server_up(timeout: float = 1.5) -> bool:
    try:
        requests.get(CHECK_URL, timeout=timeout)
        return True
    except (requests.ConnectionError, requests.Timeout):
        return False


def _start_server(detach: bool = True) -> subprocess.Popen:
    """
    Startet `ollama serve` (falls nicht aktiv) und gibt den Prozess zur√ºck.
    L√§uft er schon, wird `None` zur√ºckgegeben.
    """
    if _is_server_up():
        return None

    # `ollama serve` h√§ngt sich nicht auf STDOUT ‚áí gleich in den Hintergrund schicken
    kwargs = {"stdout": subprocess.DEVNULL, "stderr": subprocess.DEVNULL}
    cmd = ["ollama", "serve"]
    proc = subprocess.Popen(cmd, **kwargs) if detach else subprocess.Popen(cmd)

    # kleines Polling, bis Server Ports offen sind
    for _ in range(15):
        if _is_server_up():
            return proc
        time.sleep(0.6)

    raise RuntimeError("Ollama-Server konnte nicht gestartet werden.")


def _warm_up(model: str) -> None:
    """
    Ein schneller Prompt √ºber die REST-API l√§dt das Modell in den RAM.
    Schl√§gt der Call fehl, wird nur gewarnt ‚Äì das Haupt¬≠programm l√§uft weiter.
    """
    try:
        resp = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={"model": model, "prompt": "ping", "stream": False},
            timeout=30,
        )
        resp.raise_for_status()
        _ = resp.json()["response"]      # Antwort kommt nur bei success
    except Exception as exc:             # breche nicht ab, nur Hinweis
        print(f"‚ö†Ô∏è  Warm-up √ºbersprungen ({exc})")


def ensure_model(model: str = "llama3.1") -> None:
    """
    Pr√ºft, ob das Modell bereits auf der Platte liegt;
    l√§dt es andernfalls per `ollama pull <model>`.
    """
    resp = requests.get(CHECK_URL, timeout=3).json()
    available = {m["name"] for m in resp.get("models", [])}
    if model not in available:
        print(f"üì¶  Lade Modell '{model}' ‚Ä¶")
        subprocess.run(["ollama", "pull", model], check=True)


def prepare_ollama(model: str = "llama3.1") -> None:
    _start_server()
    ensure_model(model)
    _warm_up(model)
    print("‚úÖ  Ollama bereit.")

