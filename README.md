# Agentic AI Evaluation

This project implements a ü¶úüï∏Ô∏èLangGraph-based multi-agent evaluation system for answering questions using
LLMs, tools (e.g., Tavily Web Search), and self-reflection.

It compares the quality of initial and revised answers using LLM as a Judge evaluators.

---

## üîß Features

- üßû‚Äç‚ôÇÔ∏è **Responder** and üßû **Revisor** agents in a multi-agent LLM pipeline
- üîç **Tool-augmented reasoning** using Tavily Web Search when internal knowledge is insufficient
- ü™û **Self-reflective answering**, where agents critique and iteratively refine their responses
- ‚öñÔ∏è **LLM-as-a-judge** evaluating answers based on helpfulness, relevance, coherence, and conciseness
- ü§ù **Pairwise comparison** to determine which answer is better overall
- üìà **LangSmith tracing** for transparent run-level debugging and rich execution analytics  

---

## üóÇÔ∏è Project Structure

```text
agentic_ai_evaluation/
‚îÇ
‚îú‚îÄ‚îÄ main.py # Entry point: runs question-answer-evaluation pipeline
‚îú‚îÄ‚îÄ ollama_manager.py # Setup of Ollama backend
‚îú‚îÄ‚îÄ load_data.py # Loads and samples questions
‚îú‚îÄ‚îÄ chains.py # Defines LLM agents (responder and revisor)
‚îú‚îÄ‚îÄ schemas.py # Defines structured outputs and tool schemas
‚îú‚îÄ‚îÄ tool_executor.py # Wraps Tavily search tool for LangGraph
‚îú‚îÄ‚îÄ evaluator.py # Uses GPT-4o-mini to evaluate answer quality
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ results.ipynb # Notebook to view results
‚îÇ   ‚îî‚îÄ‚îÄ results.json # Output file containing evaluation results
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ hotpotqa_subset_20250101_010101.json # HotpotQA Sample questions
‚îÇ   ‚îî‚îÄ‚îÄ my_questions.json # Custom dataset with own questions
‚îî‚îÄ‚îÄ log/
    ‚îî‚îÄ‚îÄ run20250101_010101.log # Log files
```

---

## üõ†Ô∏è How It Works

1. üì• **Load questions** from a small sample of the HotpotQA dataset
2. üßû‚Äç‚ôÇÔ∏è **Responder agent** generates an initial answer using internal knowledge or Tavily web search
3. üßû **Revisor agent** critiques and improves the initial response using new context or tool results
4. ‚öñÔ∏è **LLM evaluator** scores both answers on multiple criteria and performs a pairwise comparison
5. üíæ **Save results** to `results.json` for analysis or reporting
---

## üß∞ Technologies Used

- üêç [Python 3.11](https://www.python.org/downloads/release/python-3110/)
- ü¶úüîó [LangChain](https://python.langchain.com)
- ü¶úüï∏Ô∏è [LangGraph](https://langchain-ai.github.io/langgraph/concepts/why-langgraph/)
- ü¶úüî® [LangSmith](https://docs.smith.langchain.com/)
- üß†  [OpenAI API](https://openai.com/index/openai-api/)
- ü§ó [Hugging Face Datasets](https://huggingface.co/docs/datasets)
- üîç [Tavily](https://www.tavily.com)

## üöÄ Installation

1. Clone the repository:
```bash
git clone https://github.com/JeanMarcGaller/agentic_ai_evaluation.git
```

2. Install dependencies
```bash
pip install -r requirements.txt
```

3. Set up environment variables in a .env file and add your API keys:
```text
# OpenAI + Tavily keys
OPENAI_API_KEY=your-openai-key
TAVILY_API_KEY=your-tavily-key

# LangSmith Tracing configuration
LANGCHAIN_TRACING_V2=true
LANGCHAIN_PROJECT=agentic_ai_evaluation
LANGCHAIN_API_KEY=your-langsmith-api-key
```

4. Run the main script:
```bash
python main.py
```

Or with your own questions, stored in my_questions.json:

```bash
python main.py --questions data/my_questions.json
```

Before committing, **run**:  
```bash
black .
```  
Then run `pre-commit run --all-files` if you have the hooks installed.  


## ‚öôÔ∏è Configuration Parameters

These constants let you fine-tune a run without touching the core code.  
You can override them via environment variables or CLI flags if needed.

| Name                | Default | Meaning                                                                                              |
|---------------------|---------|------------------------------------------------------------------------------------------------------|
| `MAX_MESSAGES`      | `3`     | Hard stop for a single QA turn. One ‚Äúround‚Äù consists of **3 messages** (user ‚ûû responder ‚ûû revisor). |
| `NUM_QUESTIONS`     | `5`     | How many questions are sampled and evaluated per execution of `main.py`.                             |
| `OLLAMA_MODEL_NAME` | `qwen3:32b` | Local **Ollama** model used by the responder/revisor agents. Choose model you have pulled.           |
| `OPENAI_MODEL_NAME` | `gpt-4.1` | Remote **OpenAI** model used by the responder/revisor agents.                                        |


## üìä Example Results

| Question                                                                                                             | Responder Tool | Revisor Tool | Winner    | Helpfulness Responder | Helpfulness Revisor | Correctness Responder | Correctness Revisor | Relevance Responder | Relevance Revisor | Conciseness Responder | Conciseness Revisor | Coherence Responder | Coherence Revisor |
|----------------------------------------------------------------------------------------------------------------------|----------------|--------------|-----------|-----------------------|---------------|---------------|---------------|-------------|-------------|---------------|---------------|-------------|-------------|
| Who was appointed to the board of supervisors first, Jeff Sheehy or Ed Lee?                                          | N              | N            | Responder | Y                     | Y             | Y             | Y             | Y           | Y           | Y             | Y             | Y           | Y           |
| Who was the electoral division that James Tully represented in 1928 named after?                                     | Y              | N            | Revisor   | Y                     | Y             | Y             | Y             | Y           | Y           | Y             | Y             | Y           | Y           |
| At what school is the individual who was awarded the 2012 Nobel Prize in Physics a professor?                        | N              | N            | None      | Y                     | Y             | Y             | Y             | Y           | Y           | Y             | Y             | Y           | Y           |
| Who played the female lead in a 2007 Indian Telugu film...?                                                          | Y              | N            | Revisor   | N                     | Y             | N             | Y             | N           | Y           | N             | Y             | N           | Y           |


---

## ‚ö†Ô∏è Known Issues & Limitations
- High latency due to the responder‚Äìrevisor cycle and the LLM-as-a-Judge evaluation
- Evaluation is expensive  
- In most cases, the initial response is already quite good
- Dataset questions are often generic, making them hard to interpret
- Yes/No evaluators offer limited insight; a graded score would likely be more informative, 
but attempts to implement such scoring have so far been unsuccessful






---

## üß™ Customization Ideas
- Replace HotpotQA with NaturalQuestions or WebQuestions
- Add support for more tools (e.g., arXiv API, Wikipedia search)
- Implement AsyncOpenAI-Client and retry mechanism
---

## üìö Citation

This project is inspired in part by the paper:

> Meng, Z., Dziri, N., Choudhury, M., Choi, Y., & Khashabi, D. (2023).  
> **Reflexion: Language Agents with Verbal Reinforcement Learning**.  
> arXiv: [2303.11366](https://arxiv.org/abs/2303.11366)

---

## üìÑ Attribution

This project includes components adapted from:


- [LangGraph Reflexion Tutorial](https://langchain-ai.github.io/langgraph/tutorials/reflexion/reflexion/) by the LangChain team,  
 used under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).

- [Edan Marco](https://github.com/emarco177), used under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).  
  Original codebase: [LangGraph Reflexion Agent](https://github.com/emarco177/langgraph-course)

---

## üìù License

This project is licensed under the [Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0).
See the [LICENSE](./LICENSE) file for full license text and attribution details for third-party code reused or adapted in this project, including:
- LangGraph Reflexion Tutorial (LangChain Team)
- LangGraph Reflexion Agent (Edan Marco)
---

## üì¨ Contact
This repository is part of an academic project and is provided for demonstration and review purposes only.  
If you are interested in this work, have questions or recommendations, feel free to contact me via email: 

Jean-Marc Galler

üìß jeanmarc.galler [at] students [dot] fhnw [dot] ch
